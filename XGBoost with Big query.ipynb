{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EX4_207024878.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1SPOzyXgNcV7"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "ex_path = '/content/drive/My Drive/EX_4_clinic_ds/'\n",
        "train_set_path = ex_path+'train.csv'\n",
        "test_set_path = ex_path+'test.csv'"
      ],
      "metadata": {
        "id": "u9WJW0P3QZjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.The data:"
      ],
      "metadata": {
        "id": "lsbFgSdvRdFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #load a CSV files:\n",
        "\n",
        "train_data = pd.read_csv(train_set_path)\n",
        "test_data = pd.read_csv(test_set_path)"
      ],
      "metadata": {
        "id": "GAyqUwfNNm9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some of the dtyps are 'object' and they are sopposed to be boolean - convert dtypes:\n",
        "object_cols_train = train_data.select_dtypes(include=['object']).columns\n",
        "train_data[object_cols_train] = train_data[object_cols_train].astype('bool')\n",
        "test_data[object_cols_train] = test_data[object_cols_train].astype('bool')"
      ],
      "metadata": {
        "id": "sT99ayeHlbAw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.Prediction model:"
      ],
      "metadata": {
        "id": "qT1RFmhzRflA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_fit_and_compute_rmse(train, test, imputer):#x_train, x_test, y_train, y_test, imputer)\n",
        "\n",
        "  train_imputed = train.copy()\n",
        "  test_imputed = test.copy()\n",
        "  train_nan_outcome_indices = train[train['apgar5'].isna()].index\n",
        "  test_nan_outcome_indices = test[test['apgar5'].isna()].index\n",
        "\n",
        "  if imputer != None:\n",
        "    train_imputed_vals = imputer.fit_transform(train)\n",
        "    train_imputed.loc[:, :] = train_imputed_vals\n",
        "    test_imputed_vals = imputer.transform(test) \n",
        "    test_imputed.loc[:, :] = test_imputed_vals\n",
        "    \n",
        "\n",
        "  train_imputed.drop(train_nan_outcome_indices, inplace=True) #eliminate observations without outcome - it's not good to train model on imputed outcomes\n",
        "  test_imputed.drop(test_nan_outcome_indices, inplace=True) \n",
        "  x_train, y_train = train_imputed.drop(['apgar5'],axis=1), train_imputed['apgar5']\n",
        "  x_test, y_test = test_imputed.drop(['apgar5'],axis=1), test_imputed['apgar5']\n",
        "  \n",
        "\n",
        "  #train model and hyperparameter tuning\n",
        "\n",
        "\n",
        "  params = {\n",
        "        # 'min_child_weight': [1, 5], too much parameters takes to long and make colab to crush\n",
        "        # 'gamma': [0.5, 1, 5],\n",
        "        'subsample': [0.6, 1.0],\n",
        "        # 'colsample_bytree': [0.6, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }\n",
        "\n",
        "  kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "  gs = GridSearchCV(xgb.XGBRegressor(eval_metric='rmse'), params, n_jobs=-1, cv=kfold)\n",
        "  gs.fit(X=x_train, y=y_train)\n",
        "  reg = gs.best_estimator_\n",
        "  #get rmse\n",
        "  train_rmse = np.sqrt(mean_squared_error(y_train, reg.predict(x_train)))\n",
        "  test_rmse = np.sqrt(mean_squared_error(y_test, reg.predict(x_test)))\n",
        "  return train_rmse, test_rmse"
      ],
      "metadata": {
        "id": "Vp2_HNdKQfKk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.Imputation:"
      ],
      "metadata": {
        "id": "OQQxITHgRXyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict = {'imputation method':[], 'rmse train':[], 'rmse test':[]}"
      ],
      "metadata": {
        "id": "S493QnwqLcLw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. No imputation, leaving missing data as is. XGBoost can handle missing data"
      ],
      "metadata": {
        "id": "ufm-T3sKRrPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_rmse, test_rmse = impute_fit_and_compute_rmse(train = train_data, test=test_data, imputer=None)\n",
        "result_dict['imputation method'].append('No imputation - xgb will handle it')\n",
        "result_dict['rmse train'].append(train_rmse)\n",
        "result_dict['rmse test'].append(test_rmse)"
      ],
      "metadata": {
        "id": "P1aYx82jzD4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Drop rows with missing data.\n"
      ],
      "metadata": {
        "id": "XplEQ9iXRsvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_no_na = train_data.dropna()\n",
        "test_data_no_na = test_data.dropna()\n",
        "\n",
        "train_rmse, test_rmse = impute_fit_and_compute_rmse(train = train_data_no_na, test=test_data_no_na, imputer=None)\n",
        "result_dict['imputation method'].append('drop NA')\n",
        "result_dict['rmse train'].append(train_rmse)\n",
        "result_dict['rmse test'].append(test_rmse)"
      ],
      "metadata": {
        "id": "5lcqqA_agUN1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Mean for continous, mode (most frequent) for categorical\n"
      ],
      "metadata": {
        "id": "Ca88WNstRxpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#impute mode for categorical\n",
        "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
        "categorical_cols = train_data.select_dtypes(include=['bool']).columns\n",
        "train_data_mode_imputed = train_data.copy()\n",
        "train_data_mode_imputed[categorical_cols] = imputer_mode.fit_transform(train_data_mode_imputed[categorical_cols]*1) #*1 to turn it into 0,1\n",
        "test_data_mode_imputed = test_data.copy()\n",
        "test_data_mode_imputed[categorical_cols] = imputer_mode.transform(test_data_mode_imputed[categorical_cols]*1)\n",
        "\n",
        "\n",
        "#add impute mean for continous:\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "continous_cols = train_data.select_dtypes(include=['int64', 'float64']).columns\n",
        "train_data_mean_mode_imputed = train_data_mode_imputed.copy()\n",
        "train_data_mean_mode_imputed[continous_cols] = imputer_mean.fit_transform(train_data_mode_imputed[continous_cols])\n",
        "test_data_mean_mode_imputed = test_data_mode_imputed.copy()\n",
        "test_data_mean_mode_imputed[continous_cols] = imputer_mean.transform(test_data_mean_mode_imputed[continous_cols])\n",
        "\n",
        "#fit and make prediction\n",
        "train_rmse, test_rmse = impute_fit_and_compute_rmse(train = train_data_mean_mode_imputed, test=test_data_mean_mode_imputed, imputer=None)\n",
        "result_dict['imputation method'].append('mean and mode imputation')\n",
        "result_dict['rmse train'].append(train_rmse)\n",
        "result_dict['rmse test'].append(test_rmse)"
      ],
      "metadata": {
        "id": "MOh7ZafmR04z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Median for continous, mode (most frequent) for categorical\n"
      ],
      "metadata": {
        "id": "R3Tt8p73R20Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#add impute median for continous:\n",
        "imputer_median = SimpleImputer(strategy='median')\n",
        "train_data_median_mode_imputed = train_data_mode_imputed.copy()\n",
        "train_data_median_mode_imputed[continous_cols] = imputer_median.fit_transform(train_data_median_mode_imputed[continous_cols])\n",
        "test_data_median_mode_imputed = test_data_mode_imputed.copy()\n",
        "test_data_median_mode_imputed[continous_cols] = imputer_mean.transform(test_data_median_mode_imputed[continous_cols])\n",
        "\n",
        "#fit and make prediction\n",
        "train_rmse, test_rmse = impute_fit_and_compute_rmse(train = train_data_median_mode_imputed, test=test_data_median_mode_imputed, imputer=None)\n",
        "result_dict['imputation method'].append('median and mode imputation')\n",
        "result_dict['rmse train'].append(train_rmse)\n",
        "result_dict['rmse test'].append(test_rmse)"
      ],
      "metadata": {
        "id": "sTI95wPdR2Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. kNN imputation\n"
      ],
      "metadata": {
        "id": "cQ6E1RgAR10v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "sample_size = 50000\n",
        "for k in (2, 4, 8, 16, 32, 64, 128):\n",
        "  train_rmse, test_rmse = impute_fit_and_compute_rmse(train = train_data[:sample_size], test=test_data[:sample_size], imputer = KNNImputer(n_neighbors=k))\n",
        "  result_dict['imputation method'].append('knn k={}'.format(k))\n",
        "  result_dict['rmse train'].append(train_rmse)\n",
        "  result_dict['rmse test'].append(test_rmse) "
      ],
      "metadata": {
        "id": "UqcNCQZDR1hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Iterative imputation\n"
      ],
      "metadata": {
        "id": "OmWq1c2aR1UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "train_rmse, test_rmse = impute_fit_and_compute_rmse(train = train_data[:sample_size], test=test_data[:sample_size], imputer = IterativeImputer(max_iter=10))\n",
        "result_dict['imputation method'].append('Iterative imputation')\n",
        "result_dict['rmse train'].append(train_rmse)\n",
        "result_dict['rmse test'].append(test_rmse)"
      ],
      "metadata": {
        "id": "eW15NW4jR-RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. You can add your others imputations if you’d like to.\n"
      ],
      "metadata": {
        "id": "rL3OnJ49R-mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fancyimpute"
      ],
      "metadata": {
        "id": "PbMhySRPyxYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MICE:\n",
        "import fancyimpute\n",
        "\n",
        "# ignore warnings produced by iterative imputer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "MICE_imputer = fancyimpute.IterativeImputer()\n",
        "train_rmse, test_rmse = impute_fit_and_compute_rmse(train = train_data[:sample_size], test=test_data[:sample_size], imputer = MICE_imputer)\n",
        "result_dict['imputation method'].append('fancyIterative imputation')\n",
        "result_dict['rmse train'].append(train_rmse)\n",
        "result_dict['rmse test'].append(test_rmse)"
      ],
      "metadata": {
        "id": "YmMGnIwxSKJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A table where rows are imputation methods, and a column is the RMSE on the train data and on the test data"
      ],
      "metadata": {
        "id": "UM_PCAhzSKhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(result_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "64U3ZgygMe1X",
        "outputId": "19c0b75e-5171-44f1-c45c-fc3cab27400d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     imputation method  rmse train  rmse test\n",
              "0   No imputation - xgb will handle it    0.798409   0.799535\n",
              "1                              drop NA    0.763085   0.763160\n",
              "2             mean and mode imputation    0.797811   0.798478\n",
              "3           median and mode imputation    0.798126   0.798725\n",
              "4                              knn k=2    0.792664   0.866269\n",
              "5                              knn k=4    0.792532   0.866173\n",
              "6                              knn k=8    0.793541   0.866017\n",
              "7                             knn k=16    0.792297   0.865825\n",
              "8                             knn k=16    0.792297   0.865825\n",
              "9                             knn k=32    0.792172   0.865769\n",
              "10                            knn k=64    0.792494   0.865962\n",
              "11                           knn k=128    0.792241   0.867296\n",
              "12                Iterative imputation    0.792887   0.865936\n",
              "13           fancyIterative imputation    0.792887   0.865936"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad6b0f15-67aa-4ada-8008-a4f2dd4089ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imputation method</th>\n",
              "      <th>rmse train</th>\n",
              "      <th>rmse test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No imputation - xgb will handle it</td>\n",
              "      <td>0.798409</td>\n",
              "      <td>0.799535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>drop NA</td>\n",
              "      <td>0.763085</td>\n",
              "      <td>0.763160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mean and mode imputation</td>\n",
              "      <td>0.797811</td>\n",
              "      <td>0.798478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>median and mode imputation</td>\n",
              "      <td>0.798126</td>\n",
              "      <td>0.798725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>knn k=2</td>\n",
              "      <td>0.792664</td>\n",
              "      <td>0.866269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>knn k=4</td>\n",
              "      <td>0.792532</td>\n",
              "      <td>0.866173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>knn k=8</td>\n",
              "      <td>0.793541</td>\n",
              "      <td>0.866017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>knn k=16</td>\n",
              "      <td>0.792297</td>\n",
              "      <td>0.865825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>knn k=16</td>\n",
              "      <td>0.792297</td>\n",
              "      <td>0.865825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>knn k=32</td>\n",
              "      <td>0.792172</td>\n",
              "      <td>0.865769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>knn k=64</td>\n",
              "      <td>0.792494</td>\n",
              "      <td>0.865962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>knn k=128</td>\n",
              "      <td>0.792241</td>\n",
              "      <td>0.867296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Iterative imputation</td>\n",
              "      <td>0.792887</td>\n",
              "      <td>0.865936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>fancyIterative imputation</td>\n",
              "      <td>0.792887</td>\n",
              "      <td>0.865936</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad6b0f15-67aa-4ada-8008-a4f2dd4089ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ad6b0f15-67aa-4ada-8008-a4f2dd4089ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ad6b0f15-67aa-4ada-8008-a4f2dd4089ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don’t: impute training and testing indepedently (seperately). Can you think why?\n"
      ],
      "metadata": {
        "id": "OYCKq4xKSVvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - train set and test set soppused to come from the same distribution, so we need to treat them as well (with the same distribution parameters). When we are learning the coefficents for imputation, we are kind of learning the estimates of the parameters of the distribution, so we need to learn this parameters from the train set and then apply it on the test. "
      ],
      "metadata": {
        "id": "KVYDxjOBSWdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don’t: Merge the train and test data and then impute, and then split again. Can you think why?"
      ],
      "metadata": {
        "id": "Pcp36JQSSZoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - This connects to the previous answer. In addition, we want to make a model that can handel missing data when predict as part of the prediction pipeline (and impute as the parameter we learned for the train set). If we impute all at first and then train, then if we have new data with missing values how can we impute this values? if we impute without concern the train set (as in the previous question), we will treat the new data as if it were taken from another distribution, and it's wrong.  "
      ],
      "metadata": {
        "id": "l2Er01o7ShCg"
      }
    }
  ]
}